Fraud Detection using Machine Learning

# Data Pre-processing: 
Pre-processing refers to the transformations applied to your data before feeding it to the algorithm. These methods work because of the underlying assumptions of the algorithms. Label Encoding: Sometimes our data has some features which contain string value, like gender feature contain string value of male and female. Learners like logistic regression, distance-based methods such as kNN, support vector machines, tree-based methods etc. in sklearn needs numeric arrays. Features having string values cannot be handled by these learners. So, we convert these string values into numeric values like 0 and 1. Data Cleaning: Data cleaning is the process of identifying and removing inaccurate records from a dataset, table, or database and refers to recognizing unfinished, unreliable, inaccurate or non-relevant parts of the data and then restoring, remodeling, or removing the dirty or crude data. Data cleaning is very important because if there are missing values or outliers in the data then the model will not perform well on that. Outlier Removal: Suppose our data has a feature f1 having values in range 10 to 20. But some value in that feature have value 100 or 200, then it is termed as outlier. Outliers are not good for data and should have to be removed.

# Feature Selection: 
In machine learning, if we feed garbage data to the model, then the model will give garbage prediction. Garbage means the noisy data. We have many features in our data, but we don’t need that much features for modeling. So, we use feature selection for selecting only those features which have most impact on the results of the model. Feature selection enables the algorithm to train faster, it reduces the complexity of model and it reduces overfitting. Using decision tree, we can select the features which are important. As the best splits are performed early while growing the decision tree, only the features which appear till depth d (usually set to 3) in the decision tree constructed by using a small bootstrapped subset (~10%) of the data are considered important.

# Supervised Learning: 
In data mining, we search for the patterns in the labelled data. Supervised learning is where you have input variables (x) and an output variable (Y) and you use an algorithm to learn the mapping function from the input to the output. Supervised learning problems can be further grouped into regression and classification problems. 1. Classification: A classification problem is when the output variable is a category, such as “red” or “blue” or “disease” and “no disease”. 2. Regression: A regression problem is when the output variable is a real value, such as “dollars” or “weight”. Logistic Regression: Logistic Regression is one of the most used Machine Learning algorithms for binary classification. It is a simple Algorithm that you can use as a performance baseline, it is easy to implement and it will do well enough in many tasks. Logistic Regression measures the relationship between the dependent variable (our label, what we want to predict) and the one or more independent variables (our features), by estimating probabilities using it’s underlying logistic function. SVM: “Support Vector Machine” (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However, it is mostly used in classification problems. In this algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiate the two classes very well (look at the below snapshot). The learning of the hyperplane in linear SVM is done by transforming the problem using some linear algebra. This is where the kernel plays role. Decision Tree: In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. As the name goes, it uses a tree-like model of decisions. A decision tree is drawn upside down with its root at the top. Decision Tree Classifier, repetitively divides the working area(plot) into sub part by identifying lines. (repetitively because there may be two distant regions of same class divided by other as shown in image below). So when does it terminate? 1. Either it has divided into classes that are pure (only containing members of single class). 2.Some criteria of classifier attributes are met.

# Cross-Validation: 
This process of deciding whether the numerical results quantifying hypothesized relationships between variables, are acceptable as descriptions of the data, is known as validation. Generally, an error estimation for the model is made after training, better known as evaluation of residuals. Precision= TP/Tp+Fp. Tp is true positive. Fp is false positive. Class1: +ve class. Class2:—ve class. Tp is when a model says a class 1 as class 1 only i.e what it has been predicted it actually belongs to that class only. Fp means given class 2 it has been predicted as class 1. Precision basically tells you how precise you model is to correctly classify each class. Recall: Recall literally is how many of the true positives were recalled (found), i.e. how many of the correct hits were also found. Confusion Matrix: A confusion matrix is a technique for summarizing the performance of a classification algorithm. It allows easy identification of confusion between classes e.g. one class is commonly mislabeled as the other. Most performance measures are computed from the confusion matrix. A confusion matrix is a summary of prediction results on a classification problem. The number of correct and incorrect predictions are summarized with count values and broken down by each class. This is the key to the confusion matrix. The confusion matrix shows the ways in which your classification model is confused when it makes predictions.
